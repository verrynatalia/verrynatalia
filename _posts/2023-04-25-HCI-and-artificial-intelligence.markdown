---
layout: post
title:  "HCI and Articial Intelligence"
date:   2023-04-25 11:56:25 +0000
category: blog
image:
img-caption: 'Image: '
---

As many experts have mentioned, currently, the discussion around the promise or threats of general artificial intelligence can distract the public from the real issues that narrow artificial intelligence poses to society today. Artificial intelligence permeates almost all aspects of modern life in one way or another. Based on dystopian scenarios from real life “powered” by AI use –such as social media’s eco chambers [[4](#4-reference),[5](#5-reference)] and disinformation campaigns [[4](#4-reference),[5](#5-reference)] that have caused political unrest in developed and developing countries, and algorithmic biases permeating government decisions to intervene who unjustly over-surveilled and target minorities and marginalized groups [[4](#4-reference),[5](#5-reference),[6](#6-reference),[7](#7-reference)], the list is longer…– the need for assessing the immediate and potential risks and dangers of narrow AI implementation is crucial to paving the way into a future were human values continue (or should I rather say, continue aiming for) being at the center society. The development of artificial intelligence technology comes with the bright promise of augmenting our intelligence to solve issues in a world full of them. I agree with Yann LeCun’s take on separating the discussion of AI D&R from the regulation of AI-powered tools. My understanding of artificial intelligence at the technical level is extremely vague; however, as a user and as an HCI student and designer, I have great expectations and interest in finding ways in which artificial intelligence can be embedded in computing systems to support human activities. Building effective human-AI partnerships, what some scholars and expert practitioners have prosed, is, I think, the correct approach if we want to find a good balance between the driving forces that are remanents of the industrial revolution, automation, and standardization, and a more humanistic approach to issues that keeps human values at the center of the practice. A reflection on the ethics of AI, especially done by people who are at the forefront of the development of AI tools, such as Timmit Gebru et al., is necessary to demystify artificial intelligence and safeguard society into having a dogmatic view of AI-powered tools [[2](#2-reference)]. Artificial intelligence has been trained with the information humans have –knowingly or unknowingly– provided. Language models (LM), as Bender et al. discussed, have been trained with text made by humans. Conversational NLP models appear intelligent to people, and some may attribute meaning or sentient capabilities to ChatGPT or whatever other tools out there, but they are not. NLP models generate text based on string predictions. Giving consideration to how those models impact societies is extremely relevant instead of discussing the threat of the Terminator or how we will achieve immortality thanks to AI. Bender et al. invite researchers to “consider the risks and limitations of their LMs in a guided way while also considering fixes to current designs or alternative methods of achieving a task-oriented goal in relation to specific pitfalls.” [[2](#2-reference)] They mention “value sensitive design” methodologies as useful guidance [[2](#2-reference)].

Regarding Naomi S. Baron’s commentary on ChatGPT and its possible effect on student writing skills [[1](#1-reference)]. I agree with her on the risks that such tools pose to students and human creativity in general, and I think the HCI community can contribute to designing tools that enhance human thinking and performance by working with AI researchers in the design and evaluation of tools that encourages or motivates thinking instead of becoming the place to find the “correct” answer [[1](#1-reference), [3](#3-reference)]. This is discussed by Danry et al. in their proposal for AI-framed explanations that use the Socratic method to motivate users to think for themselves instead of reliving fully on what the AI tool tells them, “casual AI-explanation” [[3](#3-reference)]. I find Danry et al. work inspiring and a great example of the type of valuable contributions that HCI research can have to the R&D of AI. They are making concrete contributions to the future of explainability [[3](#3-reference)].

**References**

1. <a id='1-reference'></a> Naomi S. Baron. 2023. How ChatGPT robs students of motivation to write and think for themselves. The Conversation. Published: January 19, 2023 8.36am EST. Retrieved: April 25, 2023. [https://theconversation.com/how-chatgpt-robs-students-of-motivation-to-write-and-think-for-themselves-197875](https://theconversation.com/how-chatgpt-robs-students-of-motivation-to-write-and-think-for-themselves-197875)
2. <a id='2-reference'></a> Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . In Conference on Fairness, Accountability, and Transparency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)
3. <a id='3-reference'></a> Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. 2023. Don’t Just Tell Me, Ask Me: AI Systems that Intelligently Frame Explanations as Questions Improve Human Logical Discernment Accuracy over Causal AI explanations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 13 pages. [https://doi.org/10.1145/3544548.3580672](https://doi.org/10.1145/3544548.3580672)
4. <a id='4-reference'></a> Ihudiya Finda Ogbonnaya-Ogburu, Angela D.R. Smith, Alexandra To, and Kentaro Toyama. 2020. Critical Race Theory for HCI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1–16. [https://doi.org/10.1145/3313831.3376392](https://doi.org/10.1145/3313831.3376392)
5. <a id='5-reference'></a> Justin Jouvenal and Spencer S. Hsu. 2020. Facial recognition used to identify Lafayette Squareprotester accused of assault, The Washington Post. [https://www.washingtonpost.com/local/legal-issues/facial-recognition-protests-lafayette-square/2020/11/02/64b03286-ec86-11ea-b4bc-3a2098fc73d4_story.html](https://www.washingtonpost.com/local/legal-issues/facial-recognition-protests-lafayette-square/2020/11/02/64b03286-ec86-11ea-b4bc-3a2098fc73d4_story.html)
6. <a id='6-reference'></a> Morgan Klaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker. 2019. How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 144 (November 2019), 33 pages. DOI: [https://doi.org/10.1145/3359246](https://doi.org/10.1145/3359246)
7. <a id='7-reference'></a> Christopher Reardon. 2021. Disabled passengers were promised autonomous vehicles — they’re still waiting. The Verge. Published: Dec 20, 2021, 10:00 AM EST. Retrieved on April 25, 2023. [https://www.theverge.com/22832657/autonomous-vehicles-disabled-accessible-challenges-design](https://www.theverge.com/22832657/autonomous-vehicles-disabled-accessible-challenges-design)
---
layout: page
title:  "HCI and Articial Intelligence"
date:   2023-04-25 11:56:25 +0000
category: blog
image:
img-caption: 'Image: '
---

As many experts have mentioned, currently, the discussion around the promises or threats of general artificial intelligence can distract the public from the real issues that narrow artificial intelligence poses to society today. 

Artificial intelligence permeates almost all aspects of modern life in one way or another. Based on dystopian scenarios from real-life “powered” by AI use –such as social media’s eco chambers [[4](#4-reference),[5](#5-reference)] and disinformation campaigns [[4](#4-reference),[5](#5-reference)] that have caused political unrest in developed and developing countries, and algorithmic biases permeating government decisions to surveil and target individuals, who may, in some cases, belong to minorities and marginalized groups [[4](#4-reference),[5](#5-reference),[6](#6-reference),[7](#7-reference)], the list is longer…– the need for assessing the immediate and potential risks and dangers of narrow AI implementation is crucial to paving the way for a future where human values continue (or should I rather say, *continue aiming to be*) to be at the center society. 

The development of artificial intelligence applications comes with the bright promise of augmenting human intelligence to help us solve the multitude of issues we encounter every day. I agree with Yann LeCun's call for differentiating AI D&R (Development and Research) from AI-powered tools when it comes to discussions around regulation. My understanding of artificial intelligence at the technical level is basic; however, as an end-user of AI-powered tools and as an HCI student, I have great expectations and interest in contributing to finding ways in which artificial intelligence can be embedded in computing systems to support human activities.

Building effective human-AI partnerships, as proposed by some scholars and expert practitioners, is, I think, the correct approach if we want to find a good balance between the driving forces that are remanants of the industrial revolution (such as automation and standardization) and a more humanistic approach to issues that keep human values at the center of computing practice. Reflecting on the ethics of AI, especially by individuals who are at the forefront of AI tool development, such as Timmit Gebru et al., is necessary to demystify artificial intelligence and safeguard society from developing a dogmatic view of AI-powered tools [[2](#2-reference)].

Artificial intelligence has been trained using the information humans have provided, knowingly or unknowingly. Language models (LM), as discussed by Bender et al., have been trained on text created by humans. Conversational NLP models appear intelligent to people, and some may attribute meaning or sentient capabilities to ChatGPT or other similar tools, but they are not. NLP models generate text based on string predictions. Instead of discussing the threat of the Terminator or how AI will enable human immortality, it is highly relevant to consider how the application of these models can impact individuals and society. Bender et al. invite researchers to “consider the risks and limitations of their LMs in a guided way while also considering fixes to current designs or alternative methods of achieving a task-oriented goal in relation to specific pitfalls.” [[2](#2-reference)] They also mention the usefulness of “value sensitive design” methodologies as guidance [[2](#2-reference)].

I agree with Naomi S. Baron’s commentary on ChatGPT, its possible effect on student writing skills [[1](#1-reference)], and the risks such tools pose to students and human creativity in general. I believe the HCI community can contribute to designing tools that enhance human thinking and performance by collaborating with AI researchers in the design and evaluation of tools that encourage or motivate thinking rather than becoming a source for finding the “correct” answer [[1](#1-reference), [3](#3-reference)]. 

This is discussed by Danry et al. in their proposal for AI-framed explanations that utilize the Socratic method to motivate users to think for themselves rather than relying solely on what the AI tool provides as a “casual AI-explanation” [[3](#3-reference)]. I find the work of Danry et al. inspiring, as it exemplifies the valuable contributions that HCI research can make to the R&D of AI. They are making tangible contributions to the future of explainability [[3](#3-reference)].

**References**

1. <a id='1-reference'></a> Naomi S. Baron. 2023. How ChatGPT robs students of motivation to write and think for themselves. The Conversation. Published: January 19, 2023 8.36am EST. Retrieved: April 25, 2023. [https://theconversation.com/how-chatgpt-robs-students-of-motivation-to-write-and-think-for-themselves-197875](https://theconversation.com/how-chatgpt-robs-students-of-motivation-to-write-and-think-for-themselves-197875)
2. <a id='2-reference'></a> Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . In Conference on Fairness, Accountability, and Transparency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)
3. <a id='3-reference'></a> Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. 2023. Don’t Just Tell Me, Ask Me: AI Systems that Intelligently Frame Explanations as Questions Improve Human Logical Discernment Accuracy over Causal AI explanations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 13 pages. [https://doi.org/10.1145/3544548.3580672](https://doi.org/10.1145/3544548.3580672)
4. <a id='4-reference'></a> Ihudiya Finda Ogbonnaya-Ogburu, Angela D.R. Smith, Alexandra To, and Kentaro Toyama. 2020. Critical Race Theory for HCI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1–16. [https://doi.org/10.1145/3313831.3376392](https://doi.org/10.1145/3313831.3376392)
5. <a id='5-reference'></a> Justin Jouvenal and Spencer S. Hsu. 2020. Facial recognition used to identify Lafayette Squareprotester accused of assault, The Washington Post. [https://www.washingtonpost.com/local/legal-issues/facial-recognition-protests-lafayette-square/2020/11/02/64b03286-ec86-11ea-b4bc-3a2098fc73d4_story.html](https://www.washingtonpost.com/local/legal-issues/facial-recognition-protests-lafayette-square/2020/11/02/64b03286-ec86-11ea-b4bc-3a2098fc73d4_story.html)
6. <a id='6-reference'></a> Morgan Klaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker. 2019. How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 144 (November 2019), 33 pages. DOI: [https://doi.org/10.1145/3359246](https://doi.org/10.1145/3359246)
7. <a id='7-reference'></a> Christopher Reardon. 2021. Disabled passengers were promised autonomous vehicles — they’re still waiting. The Verge. Published: Dec 20, 2021, 10:00 AM EST. Retrieved on April 25, 2023. [https://www.theverge.com/22832657/autonomous-vehicles-disabled-accessible-challenges-design](https://www.theverge.com/22832657/autonomous-vehicles-disabled-accessible-challenges-design)
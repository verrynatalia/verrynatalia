---
layout: post
title:  "Reflections on how LLMs can impact the delivery of education in the future"
date:   2023-06-04 09:29:22 +0000
category: blog
---

#### **To what extent do you think mastery learning will impact the delivery of education in the future and how?**

Mastery learning will continue to impact the delivery of education in the future. Since its proposal in the 1960s, it has been demonstrated to be an approach that fosters learning in all kinds of educational settings [(Bloom, 1968)](#bloom-ref). Mastery learning has significantly improved education by modeling learning in a way that considers learning as a process that builds over time and as a series of steps of increasing difficulty. Mastery learning is a simple approach, yet powerful. "The theory of mastery learning predicts that students who are allowed to master prerequisites before proceeding to the next topic will perform better on subsequent topics" [(Pelánek, 2017, pp. 156-163)](#Pelanek-ref).
Even today, finding someone who believes mastery learning does not foster long-term  learning is challenging. The complaints (if you call them that way) are mainly about the challenges of implementing mastery learning in traditional educational systems. These challenges can be attributed to a need for teacher support through access to educational technology resources that would allow them to implement personalized programs.

Educational platforms like Khan Academy have demonstrated the potential of educational technology to improve access and the quality of education. It offers personalized learning paths to students of all ages. Khan Academy is an open platform where individuals can access educational content and complete assessments while receiving immediate feedback. They also work in partnership with schools in the public education system to support teachers and students. Schools partnering with Khan Academy have access to the platform, which includes content in several subject matters, practice exercises with feedback and assessments. Students can progress through the course material at their own pace while the teacher provides individualized tutoring by request or supplemental activities related to the topic being studied [(Khan Academy, 2020)](#khan-ref).
Khan Academy’s success contrasts with other platforms that generated high expectations but failed to deliver results in terms of learning gains. Massive Open Online Courses (MOOCs) became a thing when Andrew Ng and Daphne Koller from Stanford University created the online learning platform Coursera in 2012 [(Wikipedia, 2023)](#wikipedia-ref). One of the main promises the MOOC approach offered was to offer free access to high-quality education from the best academic institutions in the world. The trend toward offering open (free) access to online courses from prestigious academic institutions may have also influenced the creation of the EdX platform [(Wikipedia, 2023)](#wikipedia-ref2), another MOOC provider, and the launch of the MIT OpenCourseWare platform [(MIT OCW, 2023)](#mit-ref).
However, the hype surrounding open education, historical retention rates in Coursera MOOCs are discouraging, with only about 5 percent of students earning a credential signifying official completion of the course [(Koller, 2013)](#koller-ref). This suggests that access to education resources, even if they are multimedia and interactive, is insufficient to keep learners engaged. As such, there is a need for additional support and guidance for learners to improve engagement and retention rates in open education.
The open education movement unlocked quality education resources from top academic institutions and made them accessible to many people. One of the gains from the open education movement was the revolutionary idea of unlocking quality education from top-tier academic institutions and organizations that are unreachable to most people. Access to any course, no matter how prestigious the provider is or how out of reach the domain or subject matter is from the learner’s close reality, is not an impediment for her to pursue the course. However, the journey from enrolling to completing the course can be challenging, and many learners struggle with engagement and retention.
Low completion rates in online courses are often linked to low learner engagement, both behaviorally and psychologically. While MOOCs are designed to appeal to a broad audience, they offer users some control over their learning experience, including access to interactive content, video lectures, and assessments with immediate feedback. Some courses also include optional social activities, such as discussion boards, but may not be effectively connected to course content. In some cases, courses are designed based on the mastery learning approach, but their implementation can be problematic, with the completion or correct marks sometimes attributed to guessing or luck [(Hui, 2023)](#hui-ref). The lack of structure and guidance in many online courses may contribute to low completion rates.
Educational technology has progressed over the years. Researchers work endlessly to deliver cost-effective solutions to improve access and quality of education as well as training for the workplace. There are many reasons that caused the pitfall of massive open online courses. One of them is due to human error in the course design. Many have equated creating online courses for learning with using a learning platform as a repository of appealing multimedia and interactive content about a topic. However, for the purposes of this discussion, the focus will stay on the issues related to the limits of educational technology, as solving these problems could significantly advance the open learning movement. Today's open learning platforms are extremely limited regarding personalization, adaptation, and learning analytics.
Integrating Massive Open Online Courses (MOOCs) with Intelligent Tutoring Systems (ITSs) can address the issue of limited personalization in MOOCs. In a 20165 study by Vincent Aleven et al., Professor of Human-Computer Interaction at Carnegie Mellon University, the complementary nature of MOOCs and ITSs was leveraged to provide learning-by-doing with detailed feedback and adaptive problem selection. ITSs can guide learners through complex problem-solving, track their skill growth, and adapt to their strategies and errors. The authors also distinguished between the tutor interface, inner loop, and outer loop. The inner loop provides within-problem guidance, while the outer loop personalizes the selection of problems based on a student model. By integrating MOOCs and ITSs, we can improve the personalization and effectiveness of online learning [(Aleven, 2015)](#aleven-ref).
While Vincent Aleven et al.'s study on MOOC-ITS integration did not result in widespread implementation, it provided valuable insights and important considerations for personalized learning. One such consideration is the significance of the inner and outer loops in tutor systems, both of which rely on the learner model to varying degrees [(Aleven, 2015)](#aleven-ref). According to Robert Bodily et al., a learner model represents variables related to the learner's knowledge, interests, affect, and other cognitive dimensions inferred from their interactions with the system [(Bodily, 2018)](#bodily-ref). We can create more personalized and effective learning experiences by incorporating learner models into MOOC-ITS integration.
According to Robert Bodily et al., learner models are a central component of research areas such as intelligent tutoring systems (ITSs), artificial intelligence in education (AIED), and adaptive hypermedia (AH). The authors note that many of these systems' adaptive capabilities rely on maintaining an up-to-date model of the learner. One key role of such a model is automatically personalizing teaching or recommendations to the learner. Studies have shown that having a student model can make a system more effective in helping students learn by adapting to individual differences, such as “cognitive mastery, which is a form of individualized problem selection based on modeling individual students’ skill mastery” [(Bodily, 2018)](#bodily-ref).
Intelligent Tutoring Systems (ITS) can contribute to mastery learning, but there are additional considerations regarding the role of learner models. Pelanek and Rihak analyzed mastery criteria using simulated and real data and found that "learner modeling techniques are not fundamental for detecting mastery" when "well-specified knowledge components" are defined. However, they also noted that "learner modeling techniques are useful for discovery and refinement of knowledge components and their relations." In line with "Baker's proposal for "stupid tutoring systems, intelligent humans" – using analytics tools to inform humans and then implement relatively simple, but wellselected and well-tuned methods into computer systems." Pelanek and Rihak suggest that the "exponential moving average method is a suitable technique" which they consider a more straightforward and robust method for online decisions [(Pelanek, 2017)](#pelanek-ref).
Ritter et al. (2014) discussed the benefits of "implement mastery learning at the individual level" rather than at the class level. While in a traditional classroom setting, there may be only a few ten students, in MOOCs, there could be thousands of students. Ritter et al. (2014) agreed with Pelanek and Rihak's (2017) conclusion on the importance of well-defined knowledge components for modeling learners. They noted that "using fine-grained knowledge components seems to be a clear pedagogical advantage over class-based mastery, but it requires a change from typical classroom practice, where teachers control the pace of instruction" [(Ritter, 2014)](#ritter-ref). These highlight the importance of defining knowledge components well and directing learners to master each topic before moving to the next.
Ritter et al. (2014) examined “mastery learning as implemented in Carnegie Learning’s Cognitive Tutor” and “found that violations of mastery learning are associated with poorer student performance, especially among struggling students, and that this result is likely attributable to such violations of mastery learning.” They also found that “over time, students who are frequently allowed to skip prerequisites find themselves with more gaps in their knowledge and are thus less able to complete more advanced tasks without making errors” [(Ritter, 2014)](#ritter-ref).
MOOCs typically operate on open access to course content without requiring prerequisites or skill assessments. While this approach may be attractive to learners and course creators, it can also lead to high rates of learner attrition and poor learning outcomes.
The future of education should continue to promote open access to educational resources, with efforts to bridge academic institutions with learners. However, as artificial intelligence (AI) technology advances, the HCI and e-learning communities need to focus on enhancing the instructional quality of MOOCs to ensure that they provide effective learning experiences to all learners. The benefits of enhancing MOOCs with AI-powered techniques may include improved learner engagement, increasing retention, or personalizing learning experiences. The use of techniques such as AI-powered ITSs and AI coaches can complement the role of instructors by providing immediate feedback and personalized explanations. AI applications have the potential to augment the capacities of instructors and teachers, enabling meaningful learning experiences to occur at a large scale.
The AI research community has suggested that "complementary computing" is the right approach to the implementation of AI-powered tools, given that "coupling human and machine intelligence improves their performance" [(Vaccaro, 2019)](#vaccaro-ref). Online education should not be an exception to the complementary computing principle. Instead, it should always advocate keeping a human in the loop and see AI as a tool to augment its capacity to reach more students. According to MIT Professor Mitchel Resnick, emphasizing too much on AI tools may contribute to an undesirable "devaluation of the human dimensions of teaching" [(Resnick, 2023)](#resnik-ref).

#### **References**
1. <a id='khan-ref'></a>Khan Academy. November 19, 2020. An Introduction to Learning Science at Khan Academy. [https://blog.khanacademy.org/](https://blog.khanacademy.org/). Retrieved: May 2, 2023.
2. <a id='wikipedia-ref'></a>Wikipedia. 2023. Coursera. [https://en.wikipedia.org/wiki/Coursera](https://en.wikipedia.org/wiki/Coursera). Retrieved: May 2, 2023.
3. <a id='wikipedia-ref2'></a>Wikipedia. 2023. EdX. [https://en.wikipedia.org/wiki/EdX](https://en.wikipedia.org/wiki/EdX). Retrieved: May 2, 2023.
4. <a id='mit-ref'></a>MIT OpenCourseWare. 2023. About us. https://ocw.mit.edu/about/. Retrieved: May 2, 2023.
5. <a id='bloom-ref'></a>Bloom, B. S. (1968). Learning for Mastery. Instruction and Curriculum. Regional Education Laboratory for the Carolinas and Virginia, Topical Papers and Reprints, Number 1.
6. <a id='aleven-ref'></a>Vincent Aleven, Jonathan Sewall, Octav Popescu, Franceska Xhakaj, Dhruv Chand, Ryan Baker, Yuan Wang, George Siemens, Carolyn Rosé, and Dragan Gasevic. The Beginning of a Beautiful Friendship? Intelligent Tutoring Systems and MOOCs. 2015.
7. <a id='resnick-ref'></a>Mitchel Resnick. April 23, 2023. AI and Creative Learning: Concerns, Opportunities, and Choices. Medium: Mitchel Resnick. [https://mres.medium.com/ai-and-creative-learning-concerns-opportunitiesand-
choices-63b27f16d4d0](https://mres.medium.com/ai-and-creative-learning-concerns-opportunitiesand-
choices-63b27f16d4d0)
8. <a id='ritter-ref'></a>Steve Ritter, Michael Yudelson, Stephen E. Fancsali, and Susan R. Berman.
2016. How Mastery Learning Works at Scale. In Proceedings of the Third (2016) ACM Conference on Learning @ Scale (L@S '16). Association for Computing Machinery, New York, NY, USA, 71–79.
[https://doi.org/10.1145/2876034.2876039](https://doi.org/10.1145/2876034.2876039)
9. <a id='hui-ref'></a>Bowen Hui. 2023. Are They Learning or Guessing? Investigating Trial-and-Error Behavior with Limited Test Attempts. In LAK23: 13th International Learning Analytics and Knowledge Conference (LAK 2023), March 13–17, 2023, Arlington, TX, USA. ACM, New York, NY, USA, 12 pages. [https://doi.org/10.1145/3576050.3576068](https://doi.org/10.1145/3576050.3576068)
10. <a id='pelanek-ref'></a>Pelánek, R., & Řihák, J. (2017). Experimental analysis of mastery learning criteria. Paper presented at the 156-163. [https://doi.org/10.1145/3079628.3079667](https://doi.org/10.1145/3079628.3079667)
11. The Beginning of a Beautiful Friendship? Intelligent Tutoring Systems and MOOCs
12. <a id='bodily-ref'></a>R. Bodily, J. Kay, V. Aleven, I. Jivet, D. Davis, F. Xhakaj, & K. Verbert. 2018. Open learner models and learning analytics dashboards: A systematic review.
In LAK’18: International Conference on Learning Analytics and Knowledge,
March 7–9, 2018, Sydney, NSW, Australia. ACM, New York, NY, USA, 10
pages. https://doi.org/10.1145/3170358.3170409
13. <a id='koller-ref'></a>Daphne Koller, Andrew Ng and Zhenghao Chen. Retention and Intention in Massive Open Online Courses: In Depth. Educause. June 3, 2013. Retrieved on May 2, 2023. [https://er.educause.edu/articles/2013/6/retention-andintention-in-massive-open-online-courses-in-depth](https://er.educause.edu/articles/2013/6/retention-andintention-in-massive-open-online-courses-in-depth)
14. <a id='clark-ref'></a>Clark, R. C., & Mayer, R. E. (2016). E-learning and the science of instruction: Proven guidelines for consumers and designers of multimedia learning (Fourth;4; ed.). Wiley. [https://doi.org/10.1002/9781119239086](https://doi.org/10.1002/9781119239086)
15. <a id='hui-ref2'></a>Bowen Hui. 2023. Are They Learning or Guessing? Investigating Trial-and-Error Behavior with Limited Test Attempts. In LAK23: 13th International Learning Analytics and Knowledge Conference (LAK 2023), March 13–17, 2023, Arlington, TX, USA. ACM, New York, NY, USA, 12 pages. [https://doi.org/10.1145/3576050.3576068](https://doi.org/10.1145/3576050.3576068)
16. <a id='vaccaro-ref'></a>Vaccaro, M., & Waldo, J. (2019). The effects of mixing machine learning and human judgment: Collaboration between humans and machines does not necessarily lead to better outcomes. ACM Queue, 17(4), 19-40. [https://doi.org/10.1145/3358955.3363293](https://doi.org/10.1145/3358955.3363293)
